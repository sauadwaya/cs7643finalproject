{"cells":[{"cell_type":"code","execution_count":null,"id":"rCdRxGwEvyax","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":82343,"status":"ok","timestamp":1764309362064,"user":{"displayName":"Alfred Wong","userId":"12244993695831585524"},"user_tz":300},"id":"rCdRxGwEvyax","outputId":"21811dc1-1c98-4667-9fd2-fe1834cd5e07"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n","Collecting transformers\n","  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m870.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n","  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n","  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n","Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.33.4\n","    Uninstalling huggingface-hub-0.33.4:\n","      Successfully uninstalled huggingface-hub-0.33.4\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.21.2\n","    Uninstalling tokenizers-0.21.2:\n","      Successfully uninstalled tokenizers-0.21.2\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.53.2\n","    Uninstalling transformers-4.53.2:\n","      Successfully uninstalled transformers-4.53.2\n","Successfully installed huggingface-hub-0.36.0 tokenizers-0.22.1 transformers-4.57.3\n","Collecting pycocoevalcap\n","  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap) (2.0.10)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (2.0.2)\n","Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pycocoevalcap\n","Successfully installed pycocoevalcap-1.2\n"]}],"source":["!pip install --upgrade transformers\n","!pip install pycocoevalcap"]},{"cell_type":"code","execution_count":null,"id":"3-79VIwNnnDe","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21130,"status":"ok","timestamp":1764310960601,"user":{"displayName":"Alfred Wong","userId":"12244993695831585524"},"user_tz":300},"id":"3-79VIwNnnDe","outputId":"43e0b057-e183-49e1-d35d-c1ee7e783179"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"id":"07BQE1fQnwf6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":301,"status":"ok","timestamp":1764310962699,"user":{"displayName":"Alfred Wong","userId":"12244993695831585524"},"user_tz":300},"id":"07BQE1fQnwf6","outputId":"4cf42e50-94c8-4c5d-d3bb-ac6dae14ac44"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/DL_project\n"]}],"source":["%cd /content/drive/MyDrive/DL_project/"]},{"cell_type":"code","execution_count":null,"id":"356415a9","metadata":{"id":"356415a9"},"outputs":[],"source":["import os\n","import pickle\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","from PIL import Image\n","\n","from tqdm import tqdm_notebook, tqdm\n","\n","from torch.optim import AdamW\n","from transformers import ViTImageProcessor, AutoTokenizer, VisionEncoderDecoderModel\n","from transformers.utils import logging\n","\n","#evaluation packages\n","from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n","from pycocoevalcap.bleu.bleu import Bleu\n","from pycocoevalcap.meteor.meteor import Meteor\n","from pycocoevalcap.rouge.rouge import Rouge\n","from pycocoevalcap.cider.cider import Cider\n","from pycocoevalcap.spice.spice import Spice\n","\n","from collections import defaultdict"]},{"cell_type":"code","execution_count":null,"id":"585e2b7e","metadata":{"id":"585e2b7e"},"outputs":[],"source":["# --- Configuration ---\n","IMG_DIR = r'./Flicker8k_images'\n","CAP_DIR = r'./Flicker8k_captions'\n","ENCODER_ID = \"google/vit-base-patch16-224-in21k\"\n","DECODER_ID = \"gpt2\"\n","TOKENIZER_NAME = \"gpt2\"\n","SAVED_MODEL_PATH = r'./saved-model' # Path to load initial model\n","CHECKPOINT_DIR = r'./image-captioning-model' # Directory to save new checkpoints\n","\n","MAX_LEN = 48\n","BATCH_SIZE = 32 # Increased to match your original DataLoader\n","EPOCHS = 5\n","LR = 1e-5\n","gradient_acc_steps = 8\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":null,"id":"8dbd45d9","metadata":{"id":"8dbd45d9"},"outputs":[],"source":["# Load test_data\n","with open(os.path.join(CAP_DIR, 'test_data.pickle'), 'rb') as f:\n","    test_data = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"id":"aab6a724","metadata":{"id":"aab6a724"},"outputs":[],"source":["class Flickr8kDataset(Dataset):\n","    def __init__(self, data, tokenizer, img_processor, img_dir, max_len, test_data=False):\n","        self.tokenizer = tokenizer\n","        self.processor = img_processor\n","        self.img_dir = img_dir\n","        self.max_len = max_len\n","        self.data = []\n","\n","        # Use all captions for better training\n","\n","        for filename, captions in data.items():\n","          if test_data:\n","            self.data.append((filename, captions[0]))\n","          else:\n","              for cap in captions:\n","                  self.data.append((filename, cap))\n","\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        filename, caption = self.data[idx]\n","\n","        # --- Image Processing ---\n","        img_path = os.path.join(self.img_dir, filename)\n","        img = Image.open(img_path).convert(\"RGB\")\n","        pixel_values = self.processor(img, return_tensors='pt').pixel_values.squeeze(0)\n","\n","        # --- Caption Processing ---\n","        tokenized_output = self.tokenizer(\n","            caption,\n","            padding='max_length',\n","            truncation=True,\n","            max_length=self.max_len,\n","            return_tensors='pt'\n","        )\n","\n","        tokens = tokenized_output.input_ids.squeeze(0)\n","        attention_mask = tokenized_output.attention_mask.squeeze(0)\n","\n","        labels = tokens.clone()\n","        labels[labels == cap_tokenizer.pad_token_id] = -100\n","\n","        return {'pixel_values': pixel_values, 'labels': labels, 'attention_mask': attention_mask, 'filename': filename}"]},{"cell_type":"code","execution_count":null,"id":"3c1f9d5e","metadata":{"id":"3c1f9d5e"},"outputs":[],"source":["img_processor = ViTImageProcessor.from_pretrained(ENCODER_ID)\n","cap_tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n","cap_tokenizer.add_special_tokens({'pad_token': '<PAD>', 'bos_token': '<BOS>'})\n","\n","test_dataset = Flickr8kDataset(test_data, cap_tokenizer, img_processor, IMG_DIR, MAX_LEN, test_data=True)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"code","execution_count":null,"id":"jKnd6nvyYvvi","metadata":{"collapsed":true,"id":"jKnd6nvyYvvi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764311067011,"user_tz":300,"elapsed":26169,"user":{"displayName":"Alfred Wong","userId":"12244993695831585524"}},"outputId":"bd05cde2-6f92-4786-f70e-c399e60c90c9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["VisionEncoderDecoderModel(\n","  (encoder): ViTModel(\n","    (embeddings): ViTEmbeddings(\n","      (patch_embeddings): ViTPatchEmbeddings(\n","        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","      )\n","      (dropout): Dropout(p=0.2, inplace=False)\n","    )\n","    (encoder): ViTEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x ViTLayer(\n","          (attention): ViTAttention(\n","            (attention): ViTSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.2, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.2, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (pooler): ViTPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (decoder): GPT2LMHeadModel(\n","    (transformer): GPT2Model(\n","      (wte): Embedding(50259, 768)\n","      (wpe): Embedding(1024, 768)\n","      (drop): Dropout(p=0.1, inplace=False)\n","      (h): ModuleList(\n","        (0-11): 12 x GPT2Block(\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2Attention(\n","            (c_attn): Conv1D(nf=2304, nx=768)\n","            (c_proj): Conv1D(nf=768, nx=768)\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (crossattention): GPT2Attention(\n","            (c_attn): Conv1D(nf=1536, nx=768)\n","            (q_attn): Conv1D(nf=768, nx=768)\n","            (c_proj): Conv1D(nf=768, nx=768)\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D(nf=3072, nx=768)\n","            (c_proj): Conv1D(nf=768, nx=3072)\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n","  )\n",")"]},"metadata":{},"execution_count":9}],"source":["model = VisionEncoderDecoderModel.from_pretrained(r'./image-captioning-model/epoch_decoder_only_baseline_3')\n","model.to(DEVICE)"]},{"cell_type":"code","execution_count":null,"id":"4f7d14b8","metadata":{"id":"4f7d14b8"},"outputs":[],"source":["def evaluate(model, loader, tokenizer, device, max_len):\n","    \"\"\"\n","    Evaluates the model on a given dataset.\n","    Calculates loss and shows some generated captions, omitting the BLEU calculation.\n","    \"\"\"\n","    model.eval()\n","    generated_captions = []\n","    lab = []\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(tqdm(loader, desc=\"Evaluating\")):\n","\n","            pixel_values = batch['pixel_values'].to(device)\n","            labels = batch['labels'].to(device)\n","            decoder_attention_mask = batch['attention_mask'].to(device)\n","            filenames = batch.get('filename')\n","\n","\n","            generated_ids = model.generate(\n","                    pixel_values=pixel_values,\n","                    max_new_tokens=20,\n","                    decoder_start_token_id=cap_tokenizer.bos_token_id,\n","                    pad_token_id=cap_tokenizer.pad_token_id,\n","                    eos_token_id=cap_tokenizer.eos_token_id,\n","                    do_sample=True,\n","                    top_p=0.9,\n","                    temperature=0.7,\n","                    #num_beams=5,\n","                    length_penalty=3.0,\n","                    repetition_penalty=3.0,\n","                    min_length=5,\n","                    early_stopping=True\n","            )\n","\n","            preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n","            generated_captions.extend(preds)\n","            lab.extend(filenames)\n","\n","    return preds, generated_captions\n"]},{"cell_type":"code","source":["# get prediction\n","label, caption = evaluate(\n","      model=model,\n","      loader=test_loader,\n","      tokenizer=cap_tokenizer,\n","      device=DEVICE,\n","      max_len=MAX_LEN\n",")\n","\n","pred = dict(zip(label, caption))\n","pred = {x: [pred[x]] for x in pred}"],"metadata":{"id":"Lycj0vWAKfqk","executionInfo":{"status":"ok","timestamp":1764141330178,"user_tz":300,"elapsed":30723,"user":{"displayName":"Alfred Wong","userId":"12244993695831585524"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7d5e7400-2ee4-4c56-82e7-4c806b9e3b50"},"id":"Lycj0vWAKfqk","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['early_stopping', 'length_penalty']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","Evaluating: 100%|██████████| 32/32 [00:30<00:00,  1.04it/s]\n"]}]},{"cell_type":"code","source":["# generate evaluation metrics\n","gts = test_data\n","res = pred\n","\n","# --------------------------------------------------------\n","# Evaluate metrics\n","# --------------------------------------------------------\n","print(\"Evaluating metrics...\\n\")\n","\n","scorers = [\n","    (Bleu(4), [\"BLEU-1\", \"BLEU-2\", \"BLEU-3\", \"BLEU-4\"]),\n","    (Meteor(), \"METEOR\"),\n","    (Rouge(), \"ROUGE-L\"),\n","    (Cider(), \"CIDEr\"),\n","    #(Spice(), \"SPICE\"),\n","]\n","\n","final_scores = {}\n","\n","for scorer, method in scorers:\n","    print(f\"Computing {method}...\")\n","\n","    score, scores = scorer.compute_score(gts, res)\n","\n","    if isinstance(method, list):  # BLEU returns 4 numbers\n","        for m, s in zip(method, score):\n","            final_scores[m] = s\n","            print(f\"{m}: {s:.4f}\")\n","    else:\n","        final_scores[method] = score\n","        print(f\"{method}: {score:.4f}\")\n","\n","print(\"\\n=== Final Results ===\")\n","for k, v in final_scores.items():\n","    print(f\"{k}: {v:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PxtvihL0jQPS","executionInfo":{"status":"ok","timestamp":1764141360308,"user_tz":300,"elapsed":20015,"user":{"displayName":"Alfred Wong","userId":"12244993695831585524"}},"outputId":"abf60bcc-567d-4b82-c91b-45f7cc95a5d8"},"id":"PxtvihL0jQPS","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating metrics...\n","\n","Computing ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4']...\n","{'testlen': 19361, 'reflen': 15680, 'guess': [19361, 18361, 17361, 16361], 'correct': [8022, 214, 6, 0]}\n","ratio: 1.2347576530611457\n","BLEU-1: 0.4143\n","BLEU-2: 0.0695\n","BLEU-3: 0.0119\n","BLEU-4: 0.0000\n","Computing METEOR...\n","METEOR: 0.1640\n","Computing ROUGE-L...\n","ROUGE-L: 0.2467\n","Computing CIDEr...\n","CIDEr: 0.1170\n","\n","=== Final Results ===\n","BLEU-1: 0.4143\n","BLEU-2: 0.0695\n","BLEU-3: 0.0119\n","BLEU-4: 0.0000\n","METEOR: 0.1640\n","ROUGE-L: 0.2467\n","CIDEr: 0.1170\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1-pjuvtpPA7rUwnWA1kQbLMMTOiQDR_Pj","timestamp":1760752573551},{"file_id":"1oyhN7auBfSPkq4Aly6iCNCtHwXDRkstO","timestamp":1760391364253}],"runtime_attributes":{"runtime_version":"2025.07"}},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":5}
